import csv

# Batch 6: Questions 31-60 (Quant & Fixed Income)
new_entries = [
    {
        "Question": "How would you design a mean-reversion trading strategy? What statistical tests would you use to validate mean reversion, and how would you avoid look-ahead bias?",
        "Reasoning": "<reason>\nMean reversion assumes prices return to a historical average (Ornstein-Uhlenbeck process). Design: Buy when z-score < -2, Sell when z-score > 2. Test using Augmented Dickey-Fuller (ADF) or Hurst Exponent (H < 0.5).\n</reason>\n<exception>\nStatistical tests often fail in non-stationary markets. A price can look mean-reverting for 10 years and then trend (breakout) forever, bankrupting the strategy (Martingale risk). Look-ahead bias: calculating z-score using the *whole sample* mean instead of the *rolling* mean available at time t.\n</exception>\n<tension>\nThis is a tension between 'Stationarity Assumption' and 'Regime Shift Reality'.\n</tension>\n<categorization>\nThis is a 'Algorithmic Trading' problem.\n</categorization>\n<deconstruction>\nMost mean reversion strategies are actually providing liquidity. You are paid for taking the other side of a large order. The 'mean' is just the equilibrium price.\n</deconstruction>\n<conclusion>\nUse ADF on *rolling windows*. Ensure the Half-Life of mean reversion is short enough to cover trading costs. Strict stop-losses are mandatory to survive regime shifts.\n</conclusion>"
    },
    {
        "Question": "Explain the methodology of principal component analysis (PCA) applied to a covariance matrix. How would you use PCA to reduce dimensionality in factor models?",
        "Reasoning": "<reason>\nPCA finds the 'Eigenvectors' (Principal Components) of the covariance matrix. These are orthogonal vectors that explain the maximum variance. PC1 is usually the 'Market', PC2 'Rates/Cluster'. You keep the top K components that explain 95% of variance.\n</reason>\n<exception>\nPCA components are statistical artifacts, not fundamental drivers. PC3 might explain variance but have no economic meaning. Also, PCA assumes linear relationships. Non-linear dependencies (tail events) are lost in dimensionality reduction.\n</exception>\n<tension>\nThis is a tension between 'Data Compression' and 'Interpretability'.\n</tension>\n<categorization>\nThis is a 'Machine Learning in Finance' problem.\n</categorization>\n<deconstruction>\nUsing PCA for risk factors avoids multicollinearity (unlike fundamental factors). However, it creates 'Stable' factors that are unstable out-of-sample.\n</deconstruction>\n<conclusion>\nUse PCA to identify the *number* of latent factors, but rotate them (Varimax) or map them to economic variables for interpretation. Don't trade a black box PC.\n</conclusion>"
    },
    {
        "Question": "A hedge fund uses machine learning to predict asset returns. What are the key challenges (overfitting, data leakage, regime change), and how would you address them?",
        "Reasoning": "<reason>\nML models are powerful pattern matchers. Key challenge: Financial data has low Signal-to-Noise ratio. Models memorize noise (overfitting). Data leakage (using future data in training, e.g., normalizing with full-sample std dev) destroys out-of-sample performance.\n</reason>\n<exception>\nThe biggest risk is 'Non-Stationarity' (Regime Change). A model trained on 2010-2020 (Bull Market) fails in 2022 (Inflation/Bear). Finance is not Physics; the rules change.\n</exception>\n<tension>\nThis is a tension between 'Model Complexity' (Deep Learning) and 'Data Scarcity/Stationarity'.\n</tension>\n<categorization>\nThis is a 'AI in Finance' problem.\n</categorization>\n<deconstruction>\nAddress overfitting with 'Regularization' (L1/L2) and 'Purged K-Fold Cross Validation' (removing data overlap between train/test sets). Address regime change with 'Online Learning' or 'Ensemble Methods'.\n</deconstruction>\n<conclusion>\nPrioritize 'Robustness' over 'Accuracy'. A simple linear model often beats a neural net out-of-sample because it overfits less. Use 'Walk-Forward' testing.\n</conclusion>"
    },
    {
        "Question": "How would you construct a factor model using linear regression? What diagnostic tests would you perform to ensure model validity?",
        "Reasoning": "<reason>\nRegress asset returns ($R_i$) against factor returns ($F_j$). $R_i = \\alpha + \\beta_1 F_1 + ... + \\epsilon$. Use OLS. Diagnostics: R-squared (fit), t-stats (significance), Durbin-Watson (autocorrelation), and VIF (Variance Inflation Factor for multicollinearity).\n</reason>\n<exception>\nOLS assumes homoscedasticity (constant variance of errors). Financial errors are heteroscedastic (volatility clustering). OLS betas will be unbiased but standard errors will be wrong. Also, factors might be endogenously determined.\n</exception>\n<tension>\nThis is a tension between 'Blue' (Best Linear Unbiased Estimator) assumptions and 'Financial Time Series' properties.\n</tension>\n<categorization>\nThis is a 'Econometrics' problem.\n</categorization>\n<deconstruction>\nUse Newey-West standard errors to correct for heteroscedasticity and autocorrelation. Or use 'Rolling Regression' to capture time-varying betas.\n</deconstruction>\n<conclusion>\nLinear regression is the baseline. Check residuals for normality and serial correlation. If residuals have structure, your model is missing a factor.\n</conclusion>"
    },
    {
        "Question": "Describe the concept of bootstrapping in financial analysis. How would you use bootstrap methods to estimate confidence intervals for portfolio returns?",
        "Reasoning": "<reason>\nBootstrapping involves resampling with replacement from historical data to create thousands of 'synthetic' histories. It preserves the distribution of the original data without assuming Normality. You calculate the metric (Sharpe) on each sample to build a distribution.\n</reason>\n<exception>\nStandard bootstrap destroys time-dependency (autocorrelation/volatility clustering). If returns are serially correlated (momentum), shuffling them breaks the pattern. You need 'Block Bootstrap' (resampling chunks of time) to preserve local structure.\n</exception>\n<tension>\nThis is a tension between 'Sample Size' and 'Structural Integrity'.\n</tension>\n<categorization>\nThis is a 'Statistical Inference' problem.\n</categorization>\n<deconstruction>\nBootstrapping assumes the past distribution represents the future population. It cannot predict a 'Black Swan' that wasn't in the historical sample.\n</deconstruction>\n<conclusion>\nUse 'Stationary Block Bootstrap' with optimal block length. It provides robust confidence intervals for non-normal data where parametric formulas fail.\n</conclusion>"
    },
    {
        "Question": "What is the difference between time-series cross-validation and cross-validation in machine learning? Why is time-series cross-validation critical for backtesting?",
        "Reasoning": "<reason>\nStandard CV (K-Fold) shuffles data randomly. This works for images (independent). Time-Series CV must respect chronology. You train on $T_{1..k}$ and test on $T_{k+1}$. You cannot train on 2020 and test on 2019.\n</reason>\n<exception>\nIf you use standard CV in finance, you introduce 'Look-Ahead Bias'. The model learns relationships from the future (e.g., volatility in 2020) to predict the past. Results will be impossibly good.\n</exception>\n<tension>\nThis is a tension between 'Data Efficiency' (Using all data) and 'Causal Causality' (Time arrow).\n</tension>\n<categorization>\nThis is a 'Backtesting Methodology' problem.\n</categorization>\n<deconstruction>\nUse 'Walk-Forward' or 'Expanding Window' CV. Train on years 1-5, test on 6. Train 1-6, test 7. This mimics the actual trading experience.\n</deconstruction>\n<conclusion>\nNever use randomized K-Fold for time series. It is the cardinal sin of backtesting. Always preserve the temporal order.\n</conclusion>"
    },
    {
        "Question": "How would you detect structural breaks in financial time series? What methods would you use?",
        "Reasoning": "<reason>\nA structural break is a permanent shift in the mean, variance, or model parameters (e.g., after 2008). Detection: Chow Test (if break date known) or CUSUM (Cumulative Sum of errors) / Bai-Perron test (if unknown).\n</reason>\n<exception>\nDetecting breaks in real-time is hard (lag). Is it a break or just an outlier? High sensitivity leads to 'Over-fitting' (changing the model too often). Low sensitivity leads to 'Model decay' (trading a dead strategy).\n</exception>\n<tension>\nThis is a tension between 'Stability' and 'Adaptability'.\n</tension>\n<categorization>\nThis is a 'Time Series Analysis' problem.\n</categorization>\n<deconstruction>\nFinancial series are 'Locally Stationary'. Instead of binary breaks, consider 'Time-Varying Parameters' (Kalman Filter) which adapt continuously.\n</deconstruction>\n<conclusion>\nUse the Chow Test for historical analysis. For live trading, use Rolling Windows or Exponential Decay to naturally overweight recent data without explicit break detection.\n</conclusion>"
    },
    {
        "Question": "A quant model shows high in-sample performance but low out-of-sample performance. What are the potential causes, and how would you diagnose this?",
        "Reasoning": "<reason>\nClassic Overfitting. The model learned the noise, not the signal. Causes: Too many parameters, too little data, lack of regularization, or data snooping (trying 1000 models and picking the best).\n</reason>\n<exception>\nAnother cause is 'Regime Change'. The out-of-sample period might be structurally different (e.g., Zero interest rates vs High rates). The model wasn't wrong; the world changed.\n</exception>\n<tension>\nThis is a tension between 'Model Complexity' (Variance) and 'Generalization' (Bias).\n</tension>\n<categorization>\nThis is a 'Model Validation' problem.\n</categorization>\n<deconstruction>\nDiagnosis: Check the gap between Train/Test error. If huge, it's overfitting. Use 'Deflated Sharpe Ratio' to adjust for the number of trials attempted.\n</deconstruction>\n<conclusion>\nSimplify the model (Occam's Razor). Reduce features. Use 'Purged Cross-Validation'. If it still fails, the 'Alpha' was likely spurious.\n</conclusion>"
    },
    {
        "Question": "Explain the Kalman filter and its applications in finance (e.g., estimating hidden state variables, tracking alpha signals).",
        "Reasoning": "<reason>\nKalman Filter is a recursive algorithm that estimates the state of a system from noisy measurements. It predicts the state, measures the error, and updates the estimate (Bayesian updating). $State_{new} = State_{pred} + Gain * Error$.\n</reason>\n<exception>\nIt assumes linear dynamics and Gaussian noise. Markets are often non-linear and non-Gaussian. Particle Filters are needed for those cases.\n</exception>\n<tension>\nThis is a tension between 'Static Parameters' (OLS) and 'Dynamic States' (Kalman).\n</tension>\n<categorization>\nThis is a 'Signal Processing' problem.\n</categorization>\n<deconstruction>\nApplications: Calculating the 'Hedge Ratio' dynamically (Beta is the state). Creating 'Moving Averages' with optimal lag. Tracking 'Equilibrium Price' in pairs trading.\n</deconstruction>\n<conclusion>\nUse Kalman Filter for dynamic beta estimation and pairs trading spread calculation. It adapts to changing correlations faster than rolling windows.\n</conclusion>"
    },
    {
        "Question": "How would you model regime-switching behavior in asset returns? What are the use cases in portfolio construction?",
        "Reasoning": "<reason>\nUse Hidden Markov Models (HMM). Assume the market has N latent states (e.g., Bull, Bear, Sideways) with different means/variances. The model estimates the transition probabilities and the current state probability.\n</reason>\n<exception>\nHMMs are notoriously unstable to initialize. They can flip-flop rapidly between states (chatter). Also, the 'Bear' state in 2008 might look different from the 'Bear' state in 2020. History doesn't repeat exactly.\n</exception>\n<tension>\nThis is a tension between 'Single-State Models' (Average) and 'Multi-State Models' (Conditional).\n</tension>\n<categorization>\nThis is a 'Quant Strategy' problem.\n</categorization>\n<deconstruction>\nUse cases: 'Risk On/Risk Off' allocation. If prob(Bear) > 50%, reduce leverage or increase hedges. It allows for non-linear allocation rules.\n</deconstruction>\n<conclusion>\nImplement HMM with 2 or 3 states (Low Vol/Bull, High Vol/Bear). Use it to modulate leverage, not to time the exact top/bottom. It acts as a 'volatility filter'.\n</conclusion>"
    },
    {
        "Question": "What is the difference between supervised and unsupervised learning? Provide examples of each applied to finance.",
        "Reasoning": "<reason>\nSupervised: Data has labels (Target Y). Goal is prediction. Example: Predicting 'Price Up/Down' (Classification) or 'Next Day Return' (Regression) based on technical indicators.\nUnsupervised: Data has no labels. Goal is structure discovery. Example: Clustering stocks (K-Means) to find sectors, or PCA for factor reduction.\n</reason>\n<exception>\nSemi-supervised or Reinforcement Learning (RL) are the frontier. In trading, the 'label' (future return) is noisy and delayed. RL learns by interacting (Action -> Reward), which fits trading better than static Supervised Learning.\n</exception>\n<tension>\nThis is a tension between 'Prediction' (Supervised) and 'Description' (Unsupervised).\n</tension>\n<categorization>\nThis is a 'Machine Learning' problem.\n</categorization>\n<deconstruction>\nSupervised Learning suffers from 'Labeling' issues (how far into the future?). Unsupervised is great for 'Diversification' (finding uncorrelated clusters).\n</deconstruction>\n<conclusion>\nUse Unsupervised (PCA/Clustering) for Portfolio Construction (Risk). Use Supervised (XGBoost/LSTM) for Alpha Signal Generation.\n</conclusion>"
    },
    {
        "Question": "How would you implement a clustering algorithm to identify similar stocks or market regimes?",
        "Reasoning": "<reason>\nUse K-Means or Hierarchical Clustering. Features: Returns correlation, Volatility, Sector, Fundamentals. Distance metric: $1 - Correlation$. Dendrograms show the hierarchy.\n</reason>\n<exception>\nK-Means requires specifying 'K' (number of clusters) beforehand. Hierarchical is better but computationally heavy ($O(N^3)$). Clusters are unstable over time. A 'Tech' stock might trade like a 'Utility' in a crash.\n</exception>\n<tension>\nThis is a tension between 'Static Classification' (GICS Sectors) and 'Dynamic Behavior' (Statistical Clusters).\n</tension>\n<categorization>\nThis is a 'Data Mining' problem.\n</categorization>\n<deconstruction>\nUse 'Hierarchical Risk Parity' (HRP). Cluster stocks, then allocate capital inversely to cluster variance. This avoids the instability of Matrix Inversion in Mean-Variance Optimization.\n</deconstruction>\n<conclusion>\nUse Hierarchical Clustering to group assets. This reveals the 'True Diversification' structure, which is often different from the sector labels.\n</conclusion>"
    },
    {
        "Question": "Explain the concept of feature importance in machine learning models. How would you identify the most influential variables in a predictive model?",
        "Reasoning": "<reason>\nFeature Importance measures how much a model's error increases if a feature is removed or permuted. Methods: 'Mean Decrease Impurity' (Trees), 'SHAP Values' (Game Theory), or 'Permutation Importance'.\n</reason>\n<exception>\nIf features are highly correlated (Multicollinearity), importance is split between them randomly. A feature might look unimportant because a proxy exists. Also, importance doesn't imply 'Causality'.\n</exception>\n<tension>\nThis is a tension between 'Prediction Power' and 'Explanatory Power'.\n</tension>\n<categorization>\nThis is a 'Explainable AI' (XAI) problem.\n</categorization>\n<deconstruction>\nSHAP (Shapley Additive exPlanations) is the gold standard. It provides local interpretability (why did the model predict UP for *this* day?) and global consistency.\n</deconstruction>\n<conclusion>\nUse SHAP values to audit the model. If the top features are nonsensical (e.g., 'Day of Week' is #1), the model is overfitting noise. Ensure top features align with economic intuition.\n</conclusion>"
    },
    {
        "Question": "What is adversarial robustness in machine learning, and why is it critical for financial models exposed to market stress?",
        "Reasoning": "<reason>\nAdversarial robustness is the model's ability to remain stable when inputs are slightly perturbed (noise/attacks). In finance, 'adversaries' are High Frequency Traders (HFT) or 'Market Makers' who sniff out your order flow, or simply extreme market noise.\n</reason>\n<exception>\nFinancial models are fragile. A small change in input (e.g., a flash crash tick) can cause a neural net to output a wild prediction (Sell everything). This leads to 'Flash Crashes'.\n</exception>\n<tension>\nThis is a tension between 'Optimization' (Fitting the training set perfectly) and 'Stability' (Handling the unknown).\n</tension>\n<categorization>\nThis is a 'AI Safety' problem.\n</categorization>\n<deconstruction>\nTrain with 'Adversarial Examples' (data with added noise). Use 'Ensemble' models to smooth out individual model fragility.\n</deconstruction>\n<conclusion>\nTest models with 'Perturbation Sensitivity'. If changing the input by 0.1% changes the output by 50%, the model is not robust. Do not deploy.\n</conclusion>"
    },
    {
        "Question": "How would you validate a machine learning model's performance using ROC curves, precision-recall, and other metrics?",
        "Reasoning": "<reason>\nAccuracy is misleading in finance (markets are noisy, ~50% baseline). Use ROC-AUC (Area Under Curve) to measure discrimination. Use Precision (True Positives / Predicted Positives) vs Recall (True Positives / Actual Positives) for trade entry.\n</reason>\n<exception>\nIn trading, 'Class Imbalance' is rare (up/down is ~50/50), but 'Magnitude' matters more than 'Direction'. A model with 40% accuracy can be profitable if the wins are 3x the losses. Classification metrics ignore P&L.\n</exception>\n<tension>\nThis is a tension between 'Statistical Metrics' (Classification) and 'Economic Metrics' (Sharpe/P&L).\n</tension>\n<categorization>\nThis is a 'Backtesting' problem.\n</categorization>\n<deconstruction>\nThe ultimate metric is the 'Sharpe Ratio' of the equity curve generated by the model's signals. ROC is just a proxy.\n</deconstruction>\n<conclusion>\nUse ROC/Precision-Recall for model selection, but validate with a 'P&L Simulator'. A high-precision model (few trades, high win rate) is often better for transaction costs.\n</conclusion>"
    },
    {
        "Question": "A portfolio manager wants to match a 10-year liability using bond holdings. How would you construct a bullet versus ladder strategy?",
        "Reasoning": "<reason>\nLiability Matching requires matching Duration. Bullet: Buy 10-year bonds. Ladder: Buy bonds maturing in 1, 2... 10 years. Both can match duration, but Ladder provides liquidity (coupons/maturities reinvested).\n</reason>\n<exception>\nBullet has high 'Convexity' at the 10y point but is sensitive to non-parallel shifts (Twists) at that tenor. Ladder diversifies 'Reinvestment Risk' and curve risk. Bullet is a targeted bet; Ladder is a structural hedge.\n</exception>\n<tension>\nThis is a tension between 'Immunization Precision' (Bullet) and 'Diversification/Liquidity' (Ladder).\n</tension>\n<categorization>\nThis is a 'Fixed Income Portfolio Management' problem.\n</categorization>\n<deconstruction>\nImmunization requires matching Duration *and* Convexity. A Bullet matches Duration but might mismatch Convexity compared to the liability. A Barbell (short + long) creates high convexity.\n</deconstruction>\n<conclusion>\nUse a Bullet for strict immunization of a single liability date. Use a Ladder if cash flow liquidity is needed or if you have no view on the yield curve shape.\n</conclusion>"
    },
    {
        "Question": "Explain the concept of key rate duration. How would you use it to manage interest rate risk in a bond portfolio?",
        "Reasoning": "<reason>\nEffective Duration assumes parallel shifts. Key Rate Duration (KRD) measures sensitivity to shifts at specific points (2y, 5y, 10y, 30y). It decomposes the risk along the curve.\n</reason>\n<exception>\nSum of KRDs = Effective Duration. Managing KRD allows you to hedge 'Curve Twists' (Flattening/Steepening). A portfolio might be duration neutral but have massive exposure to the 5y-30y spread.\n</exception>\n<tension>\nThis is a tension between 'Aggregate Risk' (Duration) and 'Structural Risk' (Curve shape).\n</tension>\n<categorization>\nThis is a 'Fixed Income Risk' problem.\n</categorization>\n<deconstruction>\nTo immunize a portfolio fully, you must match the KRD profile of the liabilities, not just the total duration.\n</deconstruction>\n<conclusion>\nCalculate KRDs. Use buckets (0-5, 5-10, 10-30). Hedge using corresponding futures (TU, FV, TY, US) to neutralize risk at each point.\n</conclusion>"
    },
    {
        "Question": "What is negative convexity in mortgages? How does prepayment risk affect the valuation of mortgage-backed securities (MBS)?",
        "Reasoning": "<reason>\nMBS holders are short a call option (Prepayment option) to the homeowner. As rates fall, homeowners refinance (prepay), so the bond is called away at par. Upside is capped. As rates rise, duration extends (extension risk). Price falls faster than a normal bond.\n</reason>\n<exception>\nThis 'Negative Convexity' makes hedging MBS difficult. The duration changes with the market (dynamic hedging needed). The option cost (OAS) must be subtracted from the yield to see value.\n</exception>\n<tension>\nThis is a tension between 'Yield' (High nominal spread) and 'Option Cost' (Prepayment risk).\n</tension>\n<categorization>\nThis is a 'MBS Valuation' problem.\n</categorization>\n<deconstruction>\nThe value depends on the 'Prepayment Model' (PSA). This relies on behavioral assumptions (do people refi efficiently?).\n</deconstruction>\n<conclusion>\nMBS offer a yield premium for selling convexity. Buy MBS if you expect low volatility (rates stable). Hedge with Swaptions (Buying Vol) to reclaim convexity.\n</conclusion>"
    },
    {
        "Question": "How would you model the term structure of interest rates using the Nelson-Siegel model? What are its applications?",
        "Reasoning": "<reason>\nNelson-Siegel fits the yield curve using 3 components: Level (Long-term), Slope (Short-term), and Curvature (Medium-term). $y(t) = \\beta_0 + \\beta_1 Exp + \\beta_2 Exp$. It provides a smooth continuous curve from discrete bond yields.\n</reason>\n<exception>\nIt is a parametric fit, not an arbitrage-free model. It can allow arbitrage opportunities. For pricing derivatives, No-Arbitrage models (Hull-White, HJM) are preferred. NS is for *fitting* and *forecasting*.\n</exception>\n<tension>\nThis is a tension between 'Descriptive Fitting' and 'Pricing Theory'.\n</tension>\n<categorization>\nThis is a 'Term Structure Modeling' problem.\n</categorization>\n<deconstruction>\nThe three betas correspond to Level, Slope, Curvature. You can forecast these factors to predict the future yield curve shape.\n</deconstruction>\n<conclusion>\nUse Nelson-Siegel (or Svensson extension) to generate the Zero Curve for discounting cash flows. Use it for relative value analysis (rich/cheap bonds on the curve).\n</conclusion>"
    },
    {
        "Question": "A corporate bond is trading at a 200 bps spread over Treasuries. How would you estimate the probability of default using the spread information?",
        "Reasoning": "<reason>\nCredit Spread $\\approx$ Prob(Default) * (1 - Recovery Rate). If Spread = 2% and Recovery = 40% (standard), then $0.02 = PD * 0.6$. $PD = 3.33\\%$ per year (Risk-Neutral Probability).\n</reason>\n<exception>\nThe spread also contains a 'Liquidity Premium' and 'Tax Premium'. The actual (Real World) PD is lower than the Risk-Neutral PD. The market demands compensation for risk aversion, not just expected loss.\n</exception>\n<tension>\nThis is a tension between 'Risk-Neutral Measure' (Pricing) and 'Physical Measure' (Actuarial).\n</tension>\n<categorization>\nThis is a 'Credit Risk' problem.\n</categorization>\n<deconstruction>\nThis is a 'Reduced Form' model approach. It treats default as a random Poisson process.\n</deconstruction>\n<conclusion>\nThe calculated PD (3.3%) is the upper bound / market-implied PD. The historical actuarial PD might be 1%. The difference is the Risk Premium.\n</conclusion>"
    },
    {
        "Question": "Explain the mechanics of a credit spread trade. How would you identify mispricings between corporate bonds and CDS?",
        "Reasoning": "<reason>\nCredit Spread Trade: Bet on spreads narrowing (Bullish credit) or widening (Bearish). Basis Trade: Arbitrage the difference between Cash Bond Spread (Z-spread) and CDS Spread. Theoretical Basis = CDS - Bond Spread should be near 0.\n</reason>\n<exception>\nIf Basis < 0 (Bond spread > CDS), the bond is cheap. Buy Bond / Buy CDS protection. You capture the difference risk-free (mostly). Friction: Funding cost of the bond (Repo) vs CDS margin.\n</exception>\n<tension>\nThis is a tension between 'Synthetic' and 'Cash' markets.\n</tension>\n<categorization>\nThis is a 'Arbitrage' problem.\n</categorization>\n<deconstruction>\nNegative Basis trades were popular in 2008. The bond was cheap because no one had cash balance sheet to buy it, while CDS required little capital. The basis reflects 'Balance Sheet Scarcity'.\n</deconstruction>\n<conclusion>\nIdentify basis mispricings. Ensure you can fund the bond position (Repo). Lock in the spread. Watch out for 'Cheapest to Deliver' mismatches in the CDS.\n</conclusion>"
    },
    {
        "Question": "What is basis risk in fixed income? Provide examples and strategies to minimize it.",
        "Reasoning": "<reason>\nBasis Risk is the risk that the hedging instrument moves differently from the asset. Example: Hedging a Corporate Bond with Treasury Futures. The spread (Basis) can widen. You are hedged against Rates, but not Credit.\n</reason>\n<exception>\nMinimization: Use instruments with higher correlation (e.g., Hedge High Yield bonds with HY ETF puts, not Treasuries). Or trade the Basis explicitly to close it.\n</exception>\n<tension>\nThis is a tension between 'Perfect Hedge' (Unavailable/Illiquid) and 'Proxy Hedge' (Liquid but Imperfect).\n</tension>\n<categorization>\nThis is a 'Risk Management' problem.\n</categorization>\n<deconstruction>\nAnother example: Futures vs Cash. The 'Cash-Futures Basis' fluctuates due to repo rates. Convergence happens at expiry, but mark-to-market pain can kill you before then.\n</deconstruction>\n<conclusion>\nAcknowledge basis risk. Measure the 'Correlation of the Basis'. Don't assume a hedge is 100%. Over-hedge or under-hedge based on the regression beta.\n</conclusion>"
    },
    {
        "Question": "How would you price a floating-rate note with caps and floors? What is the relationship between cap/floor pricing and swaption pricing?",
        "Reasoning": "<reason>\nA Floater with a Cap is a standard bond minus a 'Cap' option (Portfolio of Caplets). A Floater with a Floor is a bond plus a 'Floor' option. Price the bond at par (usually), then price the embedded options using Black's Model for Caps/Floors.\n</reason>\n<exception>\nRelationship to Swaptions: A Cap is roughly a portfolio of Payer Swaptions? No, a Caplet is an option on a *Libor* rate. A Swaption is an option on a *Swap* rate. They are linked via the yield curve correlation, but distinct.\n</exception>\n<tension>\nThis is a tension between 'Short Rate Models' (Caps) and 'Market Models' (Swaptions/LMM).\n</tension>\n<categorization>\nThis is a 'Derivatives Pricing' problem.\n</categorization>\n<deconstruction>\nIdeally, price using a model that calibrates to the Volatility Surface (SABR or LMM). Simple Black model assumes constant vol across the term.\n</deconstruction>\n<conclusion>\nDecompose the note into: Pure Floater + Long Floor - Short Cap. Price options separately. The value is Par + Value(Floor) - Value(Cap).\n</conclusion>"
    },
    {
        "Question": "A bank manager wants to immunize a bond portfolio against interest rate changes. Explain the duration matching strategy and its limitations.",
        "Reasoning": "<reason>\nImmunization: Match the Macaulay Duration of Assets and Liabilities. $D_A * A = D_L * L$. This ensures that Price effect and Reinvestment effect offset each other for small parallel shifts.\n</reason>\n<exception>\nLimitations: Only works for *small, parallel* shifts. Fails for large shifts (Convexity gap) and twists (slope changes). Requires constant rebalancing (dynamic) as duration changes with time and yield.\n</exception>\n<tension>\nThis is a tension between 'First Order Hedge' (Duration) and 'Second Order/Structural Risks' (Convexity/Shape).\n</tension>\n<categorization>\nThis is a 'ALM' (Asset Liability Management) problem.\n</categorization>\n<deconstruction>\nRedington Immunization works for flat curves. For real curves, you need to match Convexity too. $C_A > C_L$ is preferred (net positive convexity).\n</deconstruction>\n<conclusion>\nMatch Duration for the primary hedge. Match Convexity to handle volatility. Match Key Rates to handle curve reshaping. It is a maintenance process, not a set-and-forget trade.\n</conclusion>"
    },
    {
        "Question": "What is the difference between nominal yield and real yield? How would you construct a breakeven inflation analysis?",
        "Reasoning": "<reason>\nNominal Yield = Real Yield + Expected Inflation + Inflation Risk Premium. Fisher Equation. Real Yield is the return in purchasing power (TIPS yield).\n</reason>\n<exception>\nBreakeven Inflation (BEI) = Nominal Yield (Treasury) - Real Yield (TIPS). This implies what the market expects inflation to be. Example: 10Y Tsy 4%, 10Y TIPS 2% -> BEI 2%.\n</exception>\n<tension>\nThis is a tension between 'Market Implied Inflation' and 'Actual CPI Forecast'.\n</tension>\n<categorization>\nThis is a 'Macroeconomics' and 'Trading' problem.\n</categorization>\n<deconstruction>\nBEI includes the 'Inflation Risk Premium' and 'Liquidity Premium' (TIPS are less liquid). Sometimes BEI drops because TIPS are illiquid (2008), not because inflation expectations dropped.\n</deconstruction>\n<conclusion>\nTrade the Breakeven. If you think inflation > 2%, Buy TIPS / Short Nominal. Use BEI as the cleanest signal of market inflation expectations.\n</conclusion>"
    },
    {
        "Question": "Explain the concept of option-adjusted spread (OAS). How does it differ from nominal spread?",
        "Reasoning": "<reason>\nNominal Spread (Z-spread) is the yield spread over the Treasury curve. It includes credit risk + liquidity + *embedded option cost*. OAS strips out the option cost. OAS = Z-Spread - Option Cost.\n</reason>\n<exception>\nFor a callable bond (Short Call), the issuer holds the option. The investor demands a higher yield (Z-spread). The OAS is the 'pure' spread for the credit risk. Z-Spread (200bps) = OAS (150bps) + Call Option (50bps).\n</exception>\n<tension>\nThis is a tension between 'Gross Yield' (Z-spread) and 'Net/Fair Yield' (OAS).\n</tension>\n<categorization>\nThis is a 'Bond Valuation' problem.\n</categorization>\n<deconstruction>\nComparing callable vs non-callable bonds using Z-spread is wrong. Compare using OAS. If OAS is higher, the bond is cheaper relative to its credit risk.\n</deconstruction>\n<conclusion>\nUse OAS for valuation. Use Z-spread for cash flow (if you think rates won't move). OAS requires a volatility model (interest rate tree) to calculate the option value.\n</conclusion>"
    },
    {
        "Question": "How would you construct a credit model to predict corporate defaults? What variables would you include?",
        "Reasoning": "<reason>\nTwo approaches: Structural (Merton Distance to Default) and Reduced Form (Logistic Regression/Hazard). Variables: Leverage (Debt/EBITDA), Interest Coverage, Liquidity (Current Ratio), Volatility of Assets, Macro factors.\n</reason>\n<exception>\nAccounting data is lagged. Market data (Equity price, CDS) is real-time. A good model combines both (Altman Z-Score + Market signals). Machine learning (Random Forest) captures non-linear interactions.\n</exception>\n<tension>\nThis is a tension between 'Fundamental Health' and 'Market Sentiment'.\n</tension>\n<categorization>\nThis is a 'Credit Risk' problem.\n</categorization>\n<deconstruction>\nMerton Model treats equity as a Call Option on assets. Default happens if Asset Value < Debt. This links equity vol to credit risk theoretically.\n</deconstruction>\n<conclusion>\nBuild a hybrid model. Use Merton Distance-to-Default as a key feature in a Gradient Boosting classifier. Calibrate to historical default rates.\n</conclusion>"
    },
    {
        "Question": "What is a CDS basis? How would you trade the basis between CDS and cash bonds?",
        "Reasoning": "<reason>\nCDS Basis = CDS Spread - Bond Spread (Z-spread). Theoretically zero. Positive Basis (CDS > Bond): CDS is expensive. Negative Basis (CDS < Bond): Bond is cheap.\n</reason>\n<exception>\nTrade: Negative Basis -> Buy Bond, Buy CDS Protection. You earn the bond spread, pay the cheaper CDS spread, and net the difference risk-free. Positive Basis -> Sell Bond, Sell CDS Protection.\n</exception>\n<tension>\nThis is a tension between 'Synthetic' and 'Cash' liquidity/funding.\n</tension>\n<categorization>\nThis is a 'Arbitrage' problem.\n</categorization>\n<deconstruction>\nThe basis exists due to 'Funding Constraints' (Haircuts on bonds) and 'Counterparty Risk' on CDS. It is a measure of balance sheet efficiency.\n</deconstruction>\n<conclusion>\nArbitrage the basis when it diverges. Be aware that the trade consumes balance sheet (Bond inventory). Negative basis trades are popular when banks have excess cash.\n</conclusion>"
    },
    {
        "Question": "Explain the mechanics of a collateralized debt obligation (CDO). What are the risks, and how would you analyze tranches?",
        "Reasoning": "<reason>\nA CDO pools bonds/loans and slices them into tranches (Senior, Mezzanine, Equity). Cash flows follow a 'Waterfall'. Senior gets paid first; Equity gets paid last (and takes first loss).\n</reason>\n<exception>\nRisk: Correlation. If underlying assets are uncorrelated, Senior tranches are super safe. If correlation goes to 1 (systemic crisis), the whole structure fails. Rating agencies underestimated correlation in 2008 (Gaussian Copula).\n</exception>\n<tension>\nThis is a tension between 'Diversification' (Tranching) and 'Systemic Correlation'.\n</tension>\n<categorization>\nThis is a 'Structured Finance' problem.\n</categorization>\n<deconstruction>\nEquity Tranche = Long Call on the pool performance. Senior Tranche = Short Put. The structure acts as a leverage machine.\n</deconstruction>\n<conclusion>\nAnalyze the 'Attachment Point' and 'Detachment Point'. Analyze the underlying collateral correlation. High correlation hurts Senior, helps Equity (increases variance/option value).\n</conclusion>"
    },
    {
        "Question": "How would you analyze a distressed credit situation? What metrics would you focus on?",
        "Reasoning": "<reason>\nFocus on Liquidity (Runway) and Solvency (Asset coverage). Metrics: Free Cash Flow, Debt/EBITDA, Interest Coverage. Legal: Read the Indenture (Covenants, Security, seniority).\n</reason>\n<exception>\nIn distressed, standard metrics fail (EBITDA might be negative). Focus on 'Liquidation Value' vs 'Going Concern Value'. The 'Fulcrum Security' (where value breaks) determines who owns the company post-restructuring.\n</exception>\n<tension>\nThis is a tension between 'Financial Analysis' and 'Legal/Game Theory' (Restructuring).\n</tension>\n<categorization>\nThis is a 'Distressed Investing' problem.\n</categorization>\n<deconstruction>\nBuying distressed debt is buying equity cheap. You want to be the Fulcrum securityâ€”convert debt to equity and wipe out the old shareholders.\n</deconstruction>\n<conclusion>\nIdentify the Fulcrum. Estimate recovery value. Buy if Price < Recovery. Prepare for a fight (Creditor Committee).\n</conclusion>"
    }
]

filename = 'tetralemma-reasoning-train.csv'
with open(filename, 'a', newline='', encoding='utf-8') as f:
    writer = csv.DictWriter(f, fieldnames=['Question', 'Reasoning'])
    for entry in new_entries:
        writer.writerow(entry)

print(f"Appended {len(new_entries)} entries (Batch 6) to {filename}")
