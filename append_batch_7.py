import csv

# Batch 7: Questions 61-90 (Backtesting & HF Equity)
new_entries = [
    {
        "Question": "You discover that your backtesting results show a Sharpe ratio of 2.0, but live trading shows 0.8. What are the likely causes of this discrepancy?",
        "Reasoning": "<reason>\nCommon causes: Look-ahead bias, Survivorship bias, or Data Snooping (Overfitting). Also, ignoring Transaction Costs (Slippage/Commissions) or Market Impact.\n</reason>\n<exception>\nIt might be 'Alpha Decay'. The market adapted. Or 'Regime Shift'. The strategy worked in Low Vol (backtest) but live trading is High Vol. Or 'Execution Lag' (Live trading is slower than simulation).\n</exception>\n<tension>\nThis is a tension between 'Simulation Idealism' and 'Microstructure Reality'.\n</tension>\n<categorization>\nThis is a 'Quant Trading' problem.\n</categorization>\n<deconstruction>\nCheck 'Capacity'. Did you assume infinite liquidity? If you trade size, you move the price against you. This impact is invisible in backtests but real in P&L.\n</deconstruction>\n<conclusion>\nDecompose the difference. Slippage usually accounts for 50% of the drop. Overfitting accounts for the rest. Apply a 'Haircut' (e.g., 50%) to all backtests before approval.\n</conclusion>"
    },
    {
        "Question": "How would you design a proper backtesting framework to avoid look-ahead bias, survivorship bias, and data snooping?",
        "Reasoning": "<reason>\nUse 'Point-in-Time' data (PIT) to avoid look-ahead (e.g., using earnings *after* they were released). Use 'Delisted Securities' data to avoid survivorship bias. Use 'Hold-out Sets' (Test Data) never touched during training to avoid snooping.\n</reason>\n<exception>\nEven with PIT, 'Revisions' are tricky. Macro data is revised months later. You must use the 'Unrevised' first print. Data snooping is subtle; just by *viewing* the test set, you bias your next model choice.\n</exception>\n<tension>\nThis is a tension between 'Data Quality' and 'Research Discipline'.\n</tension>\n<categorization>\nThis is a 'Research Methodology' problem.\n</categorization>\n<deconstruction>\nThe best check is 'Paper Trading'. Run the model live without money for 3 months. If it correlates with the backtest, then fund it.\n</deconstruction>\n<conclusion>\nBuild a 'Time Machine' database. Query data 'As Of' a specific timestamp. Never overwrite history with corrected data.\n</conclusion>"
    },
    {
        "Question": "Explain the concept of walk-forward optimization. How would you validate a trading strategy using this method?",
        "Reasoning": "<reason>\nWalk-forward mimics real life. Optimize parameters on Window 1 (Train), trade on Window 2 (Test). Move forward. Optimize on Window 2, trade on Window 3. This creates a series of out-of-sample results.\n</reason>\n<exception>\nIt assumes parameters vary slowly. If the optimal parameter jumps wildly (Window 1: MA=50, Window 2: MA=200), the strategy is unstable. It is computationally expensive.\n</exception>\n<tension>\nThis is a tension between 'Parameter Stability' and 'Market Adaptation'.\n</tension>\n<categorization>\nThis is a 'Optimization' problem.\n</categorization>\n<deconstruction>\nIt prevents fitting to the whole history. If the Walk-Forward Equity Curve is smooth, the strategy is robust. If it is choppy, the strategy is overfitted.\n</deconstruction>\n<conclusion>\nUse Walk-Forward. Measure the 'Performance Degradation' (Out-of-Sample vs In-Sample). If OOS performance is > 50% of IS, it is passing.\n</conclusion>"
    },
    {
        "Question": "What is the difference between parametric and non-parametric bootstrapping in backtesting? When would you use each?",
        "Reasoning": "<reason>\nParametric: Fit a distribution (Normal, T) to returns, then sample from the distribution. Assumes a model. Non-parametric: Resample actual historical returns with replacement. Assumes history repeats.\n</reason>\n<exception>\nParametric allows simulating 'Black Swans' (by adjusting tail parameters) that never happened in history. Non-parametric is bound by the min/max of history. Use Parametric for Stress Testing extremes; Non-parametric for general validation.\n</exception>\n<tension>\nThis is a tension between 'Model Risk' (Parametric) and 'Historical Bias' (Non-parametric).\n</tension>\n<categorization>\nThis is a 'Simulation' problem.\n</categorization>\n<deconstruction>\nStationary Block Bootstrap (Non-parametric) preserves volatility clustering. This is usually the best default for trading strategies.\n</deconstruction>\n<conclusion>\nUse Non-Parametric Block Bootstrap for Sharpe Ratio confidence intervals. Use Parametric (EVT) for VaR/Tail Risk estimation.\n</conclusion>"
    },
    {
        "Question": "How would you stress test a trading strategy against historical market crises (e.g., 2008, COVID-19)?",
        "Reasoning": "<reason>\nReplay the strategy through specific date ranges (2008, 2020). Check Max Drawdown. Ensure the strategy survives (doesn't hit margin calls or stop-outs).\n</reason>\n<exception>\nHistorical replays are insufficient. The next crisis will look different. You need 'Synthetic Scenarios'. What if Correlation=1 but Volatility stays low? What if Rates spike + Equity crash (Stagflation)?\n</exception>\n<tension>\nThis is a tension between 'Historical Precedent' and 'Hypothetical Risk'.\n</tension>\n<categorization>\nThis is a 'Risk Management' problem.\n</categorization>\n<deconstruction>\nCorrelations break during crises. Liquidity dries up. Stress test the 'Liquidity' assumption. Assume you can't exit for 3 days. Does the strategy survive?\n</deconstruction>\n<conclusion>\nRun historical scenarios as a baseline. Then apply 'Factor Shocks' (e.g., Value factor -20%). If the strategy dies in any plausible scenario, size it down.\n</conclusion>"
    },
    {
        "Question": "What is Monte Carlo simulation, and how would you use it to estimate the probability distribution of portfolio returns?",
        "Reasoning": "<reason>\nGenerate thousands of random market paths based on drift/volatility/correlation. Revalue portfolio on each path. Plot the histogram of final values. It gives the full PDF of outcomes.\n</reason>\n<exception>\nMC depends heavily on the input assumptions (Normal vs Fat Tail). If you use Gaussian inputs, you underestimate tail risk. 'Garbage in, Garbage out'. Also, it ignores 'Structural Breaks'.\n</exception>\n<tension>\nThis is a tension between 'Stochastic Process' and 'Market Physics'.\n</tension>\n<categorization>\nThis is a 'Risk Analysis' problem.\n</categorization>\n<deconstruction>\nUse 'Historical Simulation' (resampling past days) mixed with MC to capture fat tails without assuming a distribution.\n</deconstruction>\n<conclusion>\nUse MC to estimate 'Probability of Ruin' and VaR. Use Student-t or Empirical distributions for the random generator, not Normal.\n</conclusion>"
    },
    {
        "Question": "A strategy shows high Sharpe ratio in-sample but fails out-of-sample. How would you diagnose the problem using performance attribution?",
        "Reasoning": "<reason>\nAttribution breaks down P&L by factor (Market, Sector, Style). Check if the Alpha came from a specific factor exposure (e.g., Long Momentum) that worked in the past but reversed OOS.\n</reason>\n<exception>\nMaybe the strategy was 'p-hacked'. The failure is mathematical (regression to the mean). Or 'Transaction Costs'. The OOS period had higher volatility/spreads.\n</exception>\n<tension>\nThis is a tension between 'Skill' (Alpha) and 'Luck/Factor' (Beta).\n</tension>\n<categorization>\nThis is a 'Performance Analysis' problem.\n</categorization>\n<deconstruction>\nIf the strategy has high exposure to a factor (e.g., Small Cap), and Small Caps underperformed OOS, the strategy isn't broken; the factor is just in a drawdown. If the *Residual* (Alpha) is negative, the strategy is broken.\n</deconstruction>\n<conclusion>\nStrip out the Factor Beta. Analyze the 'Idiosyncratic P&L'. If that degrades OOS, the edge is gone (arb closed or overfitted).\n</conclusion>"
    },
    {
        "Question": "Explain the concept of slippage in backtesting. How would you model realistic market impact costs?",
        "Reasoning": "<reason>\nSlippage is the difference between Expected Price (Mid/Last) and Execution Price. It includes Spread and Market Impact. Model it as: Fixed cost (Spread/2) + Variable cost (Function of Volume).\n</reason>\n<exception>\nImpact is non-linear. Trading 1% of ADV (Average Daily Volume) has small impact. Trading 10% has massive impact (Square Root Law). $Impact = k * \\sigma * \\sqrt{Size/Volume}$.\n</exception>\n<tension>\nThis is a tension between 'Paper Profits' and 'Realized Profits'.\n</tension>\n<categorization>\nThis is a 'Transaction Cost Analysis' problem.\n</categorization>\n<deconstruction>\nSlippage kills high-frequency strategies. You must simulate the 'Order Book' or use a conservative 'Square Root' model. Constant slippage assumption is dangerous.\n</deconstruction>\n<conclusion>\nUse a 'Square Root Impact Model'. Penalize larger trades. If the strategy survives 2x the estimated slippage, it is robust.\n</conclusion>"
    },
    {
        "Question": "What is transaction cost analysis (TCA), and how would you incorporate it into your backtesting framework?",
        "Reasoning": "<reason>\nTCA measures execution quality (Implementation Shortfall). Pre-trade estimation vs Post-trade measurement. Incorporate it by subtracting estimated costs from every trade in the backtest.\n</reason>\n<exception>\nTCA is dynamic. Costs are higher in high vol / low liquidity. Your backtest cost model should be a function of Volatility and Volume, not a flat bps fee.\n</exception>\n<tension>\nThis is a tension between 'Gross Alpha' and 'Net Alpha'.\n</tension>\n<categorization>\nThis is a 'Execution' problem.\n</categorization>\n<deconstruction>\nFeedback loop: High costs should signal the strategy to trade *less*. The backtest optimizer should include costs in the objective function to find the optimal turnover.\n</deconstruction>\n<conclusion>\nIntegrate a 'Cost Model' into the strategy logic. Don't just subtract costs at the end; let the strategy *see* the costs and decide if the trade is worth it.\n</conclusion>"
    },
    {
        "Question": "How would you detect and eliminate data snooping bias in your research process?",
        "Reasoning": "<reason>\nData Snooping (P-hacking) happens when you reuse the same dataset to test multiple hypotheses. Detect with 'White's Reality Check' or 'Hansen's SPA' (Superior Predictive Ability). These adjust the p-value for the number of trials.\n</reason>\n<exception>\nElimination is cultural. Preregister hypotheses. Use a 'Research Data' set and a locked 'Validation Data' set. Only run on Validation once. If it fails, discard the strategy; don't tweak it.\n</exception>\n<tension>\nThis is a tension between 'Exploration' (Trying ideas) and 'Confirmation' (Validating ideas).\n</tension>\n<categorization>\nThis is a 'Scientific Integrity' problem.\n</categorization>\n<deconstruction>\nThe more you search, the more likely you find a false positive. Use 'Deflated Sharpe Ratio' which accounts for the number of backtests run.\n</deconstruction>\n<conclusion>\nassume all high Sharpes are false positives until proven otherwise by OOS performance. Limit the number of trials per researcher.\n</conclusion>"
    },
    {
        "Question": "What is the Sharpe ratio's relationship to the information ratio? How would you compare active managers using these metrics?",
        "Reasoning": "<reason>\nSharpe = (Rp - Rf) / Vol_p. Measures absolute risk-adjusted return. Information Ratio (IR) = (Rp - Rb) / Tracking Error. Measures active return relative to a benchmark (Rb). They are related: $IR \\approx Sharpe$ if Benchmark is Cash.\n</reason>\n<exception>\nA manager can have high Sharpe (market exposure) but low IR (no alpha). Or high IR (good stock picking) but low Sharpe (high market vol). Use Sharpe for *Asset Allocation*. Use IR for *Manager Selection* (Alpha).\n</exception>\n<tension>\nThis is a tension between 'Total Risk' and 'Active Risk'.\n</tension>\n<categorization>\nThis is a 'Performance Measurement' problem.\n</categorization>\n<deconstruction>\nFundamental Law of Active Management: $IR = IC * \\sqrt{Breadth}$. High IR requires skill (IC) and opportunities (Breadth). Sharpe is just the result.\n</deconstruction>\n<conclusion>\nUse IR to judge the manager's skill in beating their specific style. Use Sharpe to judge if the fund belongs in your portfolio (does it improve the efficient frontier?).\n</conclusion>"
    },
    {
        "Question": "Explain the concept of maximum drawdown. How would you construct a strategy that minimizes drawdowns while maximizing returns?",
        "Reasoning": "<reason>\nMax Drawdown (MDD) is the largest peak-to-trough decline. It measures pain/ruin risk. To minimize MDD: Diversify, use Stop-Losses, and use 'Vol Targeting' (reduce size when vol rises).\n</reason>\n<exception>\nMinimizing MDD often reduces Compounded Returns (CAGR) because you exit too early or hold too much cash (drag). Perfect safety is zero return. The goal is optimizing Calmar Ratio (Return / MDD).\n</exception>\n<tension>\nThis is a tension between 'Capital Preservation' and 'Capital Growth'.\n</tension>\n<categorization>\nThis is a 'Risk Management' problem.\n</categorization>\n<deconstruction>\nTrend Following strategies naturally limit MDD (stops) but suffer from 'whipsaw'. Mean Reversion suffers from large MDD (catching falling knives). Combine them to smooth the curve.\n</deconstruction>\n<conclusion>\nTarget Volatility. Scale positions inversely to volatility. This keeps risk constant and naturally cuts exposure during crashes, limiting MDD.\n</conclusion>"
    },
    {
        "Question": "How would you perform a sensitivity analysis on a trading strategy to identify parameter robustness?",
        "Reasoning": "<reason>\nVary parameters (e.g., Moving Average length 40-60 instead of 50). Plot a 'Heatmap' of Sharpe Ratio. If the performance is a stable 'Plateau', it is robust. If it is a 'Spike' (only MA=50 works), it is overfitted.\n</reason>\n<exception>\nSome parameters *should* be specific (e.g., seasonality, trading hours). Sensitivity analysis might discard valid strategies that are tuned to specific market structural frequencies. But usually, broad plateaus are safer.\n</exception>\n<tension>\nThis is a tension between 'Optimization' (Peak) and 'Robustness' (Average).\n</tension>\n<categorization>\nThis is a 'Model Validation' problem.\n</categorization>\n<deconstruction>\nThe best parameter is often the 'Average' of the high-performing region, not the peak. This 'Bagging' approach reduces variance.\n</deconstruction>\n<conclusion>\nReject 'Spiky' strategies. Choose parameters from the center of the stability plateau. Better to be roughly right than precisely wrong.\n</conclusion>"
    },
    {
        "Question": "What is the Calmar ratio, and how does it compare to the Sharpe ratio for evaluating hedge fund performance?",
        "Reasoning": "<reason>\nCalmar = CAGR / Max Drawdown. It measures return per unit of 'worst-case pain'. Sharpe = Return / Volatility. Sharpe assumes risk is volatility (noise). Calmar assumes risk is drawdown (loss).\n</reason>\n<exception>\nSharpe works for Normal distributions. Calmar is better for 'Fat Tail' strategies (like Short Vol or Trend Following) where Vol is low but Drawdown can be high (or vice versa). Calmar focuses on the tail.\n</exception>\n<tension>\nThis is a tension between 'Average Deviation' (Sharpe) and 'Extreme Deviation' (Calmar).\n</tension>\n<categorization>\nThis is a 'Performance Analysis' problem.\n</categorization>\n<deconstruction>\nInvestors care more about Drawdown (psychological pain/redemption point) than Volatility. A high Calmar fund is easier to hold.\n</deconstruction>\n<conclusion>\nUse Calmar for Hedge Funds (non-normal returns). Use Sharpe for Mutual Funds/Indices (closer to normal). A Calmar > 2.0 is excellent.\n</conclusion>"
    },
    {
        "Question": "How would you analyze the return distribution of a strategy to identify skewness, kurtosis, and tail risk?",
        "Reasoning": "<reason>\nPlot the Histogram. Calculate 3rd moment (Skew) and 4th moment (Kurtosis). Check Jarque-Bera test for normality. Negative Skew = Tail Risk (Crash prone). High Kurtosis = Fat Tails (Frequent surprises).\n</reason>\n<exception>\nMoments are unstable estimators. A single outlier changes Skew/Kurtosis massively. Visual inspection (QQ Plot) and 'Downside Beta' or 'VaR' are more robust practical measures.\n</exception>\n<tension>\nThis is a tension between 'Summary Statistics' and 'Graphical/Robust Analysis'.\n</tension>\n<categorization>\nThis is a 'Statistics' problem.\n</categorization>\n<deconstruction>\nStrategies with High Sharpe often have Negative Skew (e.g., Carry, Arb). They make small steady gains and take one big loss. Don't be fooled by the Sharpe.\n</deconstruction>\n<conclusion>\nReject strategies with high negative skew unless the return premium is massive. Use QQ Plots to visualize the tails deviation from Normal.\n</conclusion>"
    },
    {
        "Question": "A long/short equity hedge fund holds 100 long positions (average position size 1.5%) and 50 short positions (average position size 2%). What is the net exposure and gross exposure? How would you interpret these metrics?",
        "Reasoning": "<reason>\nLong = 100 * 1.5% = 150%. Short = 50 * 2% = 100%. Gross Exposure = L + |S| = 250%. Net Exposure = L - |S| = 50%.\n</reason>\n<exception>\nNet 50% means the fund is directional (Long Bias). It correlates with the market. Gross 250% means high leverage (2.5x). It implies high Idiosyncratic risk and conviction.\n</exception>\n<tension>\nThis is a tension between 'Market Exposure' (Net) and 'Leverage/Alpha Opportunity' (Gross).\n</tension>\n<categorization>\nThis is a 'Hedge Fund Strategy' problem.\n</categorization>\n<deconstruction>\nA high gross/low net fund (e.g., Market Neutral) relies entirely on Alpha (Stock Picking). A moderate gross/high net fund relies on Beta.\n</deconstruction>\n<conclusion>\nThe fund is a levered Long-Biased fund (150/100). It is betting on stocks rising but trying to add alpha via shorts. Risk is high (2.5x leverage).\n</conclusion>"
    },
    {
        "Question": "A fund manager identifies a stock that is fundamentally undervalued but has been declining 15% annually. How would you determine if this is a shorting opportunity or a value trap?",
        "Reasoning": "<reason>\nValue Trap: Cheap but broken business (melting ice cube). Short Opportunity: Valuation is still too high relative to the decline, or fraud. Check 'Catalysts'. If no catalyst to unlock value, it's a trap (avoid). If earnings are deteriorating faster than price, it's a short.\n</reason>\n<exception>\nDeep Value investors buy when it hurts. Mean reversion is powerful. The decline might be 'Sentiment' (oversold). Check: Free Cash Flow yield, Debt maturity (solvency), and Insider Buying.\n</exception>\n<tension>\nThis is a tension between 'Price Momentum' (Trend) and 'Fundamental Value' (Mean Reversion).\n</tension>\n<categorization>\nThis is a 'Stock Picking' problem.\n</categorization>\n<deconstruction>\nShorting value traps is dangerous because they can be bought out (M&A). Best shorts are 'Overvalued' + 'Negative Momentum'. Cheap stocks are hard shorts.\n</deconstruction>\n<conclusion>\nDon't short on valuation alone. Short on 'Structural Decline' or 'Accounting Fraud'. If it's just 'Cheap and falling', stay away or buy Puts (defined risk).\n</conclusion>"
    },
    {
        "Question": "Explain the mechanics of shorting a stock. What are the costs (borrow costs, dividends, short squeezes), and how would you manage them?",
        "Reasoning": "<reason>\nTo short, you borrow shares from a broker (Prime) and sell them. You promise to return them later. Costs: Borrow Fee (HTB rates can be 50%+), Dividends (you pay the dividend to the lender), and Margin Interest.\n</reason>\n<exception>\nRisks: 'Buy-in' (Lender recalls shares). 'Short Squeeze' (Price spikes, margin calls force covering, driving price higher). Unlimited downside loss.\n</exception>\n<tension>\nThis is a tension between 'Negative Delta' (Profit from drop) and 'Asymmetric Risk' (Infinite loss).\n</tension>\n<categorization>\nThis is a 'Trading Mechanics' problem.\n</categorization>\n<deconstruction>\nManaging squeeze risk: Size shorts small (<2% NAV). Avoid high short interest names (>20% float). Use Options (Puts) to define risk.\n</deconstruction>\n<conclusion>\nShorting is expensive and dangerous. Only short when you have an 'Information Edge' or specific catalyst. Never short purely on valuation.\n</conclusion>"
    },
    {
        "Question": "A long/short fund has a beta of 0.3 with an information ratio of 1.5. What does this tell you about the fund's alpha generation and risk profile?",
        "Reasoning": "<reason>\nBeta 0.3 means low correlation to market (defensive). IR 1.5 is excellent (top quartile). It means the fund generates significant active return per unit of active risk. It is a high-alpha source.\n</reason>\n<exception>\nCheck the benchmark. Is Beta calculated against S&P 500? Maybe it has high beta to Tech or Momentum. Is the IR driven by a short period of luck? High IR with low Beta is the 'Holy Grail' of diversifying alternatives.\n</exception>\n<tension>\nThis is a tension between 'Systematic Exposure' (Beta) and 'Skill' (Alpha/IR).\n</tension>\n<categorization>\nThis is a 'Fund Analysis' problem.\n</categorization>\n<deconstruction>\nThis fund acts as a diversifier. Adding it to a 60/40 portfolio improves the Sharpe Ratio significantly.\n</deconstruction>\n<conclusion>\nIt is a high-quality 'Alpha Generator'. It justifies high fees because it provides a return stream uncorrelated to the market.\n</conclusion>"
    },
    {
        "Question": "How would you construct a market-neutral long/short portfolio? What constraints would you impose?",
        "Reasoning": "<reason>\nGoal: Beta = 0. Dollar Neutral ($ Long = $ Short). Method: Optimization (Mean-Variance) or Pairing (Long GM / Short Ford). Constraints: Sector Neutrality, Factor Neutrality (Size, Momentum).\n</reason>\n<exception>\nTrue neutrality is hard. Beta changes. Dollar neutral is not Beta neutral (High Beta longs vs Low Beta shorts = Net Long Beta). You must hedge Factor Betas.\n</exception>\n<tension>\nThis is a tension between 'Hedging' (Removing risk) and 'Returns' (Cost of hedging).\n</tension>\n<categorization>\nThis is a 'Portfolio Construction' problem.\n</categorization>\n<deconstruction>\nMarket Neutral funds often use leverage (gross 300%) to amplify the small alpha spread. This introduces 'Liquidity Risk' and 'Leverage Risk'.\n</deconstruction>\n<conclusion>\nImpose strict Beta (-0.1 to 0.1) and Sector constraints. Use a Multi-Factor Risk Model to strip out unwanted exposures. The return should come purely from stock selection.\n</conclusion>"
    },
    {
        "Question": "A short position in a small-cap stock suddenly spikes 50% due to a takeover announcement. How would you manage this position?",
        "Reasoning": "<reason>\nCover immediately. The takeover price is usually a floor. The thesis (structural decline) is broken by the M&A event. You lost. Take the hit.\n</reason>\n<exception>\nIf the deal faces antitrust scrutiny or financing risk, the stock might trade at a discount to the offer. You could stay short to bet on the deal breaking (Arb). But this changes the trade from 'Fundamental Short' to 'Merger Arb Short'.\n</exception>\n<tension>\nThis is a tension between 'Stop Loss discipline' and 'New Thesis formation'.\n</tension>\n<categorization>\nThis is a 'Risk Management' problem.\n</categorization>\n<deconstruction>\nUsually, in a takeover, the stock pins to the deal price. Shorting is dead money unless you have a specific reason to doubt the deal. The 'Opportunity Cost' of capital suggests exiting.\n</deconstruction>\n<conclusion>\nExit. Do not turn a failed fundamental trade into a speculative arbitrage bet. Admit defeat and redeploy capital.\n</conclusion>"
    },
    {
        "Question": "Explain the difference between statistical arbitrage and fundamental long/short investing. What are the skill sets required for each?",
        "Reasoning": "<reason>\nStat Arb: Quantitative, high frequency, short holding period (days/minutes). Relies on mean reversion, correlations, price patterns. Skill: Math, Coding, Data.\nFundamental L/S: Qualitative, longer holding (months/years). Relies on earnings, management, valuation. Skill: Accounting, Industry knowledge, Networking.\n</reason>\n<exception>\nConvergence: 'Quantamental'. Fundamental funds use alternative data (credit cards). Quant funds use fundamental factors (Quality, Value). The lines are blurring.\n</exception>\n<tension>\nThis is a tension between 'Price Action' (Technical) and 'Business Value' (Fundamental).\n</tension>\n<categorization>\nThis is a 'Hedge Fund Strategy' problem.\n</categorization>\n<deconstruction>\nStat Arb is 'Providing Liquidity' or 'Correcting Noise'. Fundamental is 'Correcting Value'. Both provide market efficiency at different timescales.\n</deconstruction>\n<conclusion>\nStat Arb requires infrastructure and speed. Fundamental requires depth and patience. Choose based on your edge (Computer vs Human).\n</conclusion>"
    },
    {
        "Question": "How would you identify pairs trading opportunities? What statistical tests would you use to validate a pair?",
        "Reasoning": "<reason>\nIdentify economically related assets (Coke/Pepsi, Gold/Silver). Check for 'Cointegration' (not just correlation). Cointegration means the spread is mean-reverting (stationary).\n</reason>\n<exception>\nCorrelation can be spurious. Cointegration (Engle-Granger test) checks if the linear combination is stationary. Historical relationship might break (structural break). Validating on backtest is not enough.\n</exception>\n<tension>\nThis is a tension between 'Historical Pattern' and 'Structural Link'.\n</tension>\n<categorization>\nThis is a 'Stat Arb' problem.\n</categorization>\n<deconstruction>\nUse the 'Spread' z-score. Trade when spread > 2 sigma. Exit at mean. Fundamental reason for the link (same supply chain) adds robustness.\n</deconstruction>\n<conclusion>\nUse Cointegration tests (ADF on the residuals). Ensure there is a fundamental reason for the pair to move together. Avoid pairs with divergent corporate actions (M&A).\n</conclusion>"
    },
    {
        "Question": "A fund wants to hedge sector exposure. Should it use index futures, short sector ETFs, or individual stock shorts? What are the trade-offs?",
        "Reasoning": "<reason>\nIndex Futures (S&P): Cheap, liquid, capital efficient. But correlation mismatch (Basis risk) if hedging a specific sector.\nSector ETFs (XLF): Better correlation. Costs borrow fee (if shorting) or expense ratio. Good balance.\nIndividual Shorts (Custom Basket): Perfect hedge (can strip out specific factors). Highest alpha potential (shorting bad companies). High cost/effort.\n</reason>\n<exception>\nShorting individual stocks introduces 'Idiosyncratic Risk' (squeeze). Futures have no idiosyncratic risk. ETFs have little.\n</exception>\n<tension>\nThis is a tension between 'Hedge Precision' and 'Cost/Complexity'.\n</tension>\n<categorization>\nThis is a 'Hedging' problem.\n</categorization>\n<deconstruction>\nUse ETFs for tactical sector hedging. Use Futures for broad market hedging. Use Custom Baskets if you have a specific short alpha thesis.\n</deconstruction>\n<conclusion>\nUse Sector ETFs (highly liquid ones). It minimizes basis risk compared to S&P futures, without the squeeze risk of single stocks.\n</conclusion>"
    },
    {
        "Question": "How would you analyze the quality of a company's earnings? What metrics and red flags would you monitor?",
        "Reasoning": "<reason>\nMetrics: Cash Conversion (OCF / Net Income). Should be > 1. Accruals Ratio. Days Sales Outstanding (DSO) trend. Inventory turnover.\n</reason>\n<exception>\nRed Flags: Rising receivables > Sales growth (Channel stuffing). Rising Inventory (Obscolescence). Capitalizing expenses (boosting income). One-time gains classified as operating. Divergence between GAAP and Non-GAAP.\n</exception>\n<tension>\nThis is a tension between 'Reported EPS' (Accounting) and 'Economic Cash Flow' (Reality).\n</tension>\n<categorization>\nThis is a 'Forensic Accounting' problem.\n</categorization>\n<deconstruction>\nLook at the footnotes. Changes in revenue recognition policy or depreciation schedules are smoking guns. 'Quality of Earnings' is about sustainability.\n</deconstruction>\n<conclusion>\nTrust Cash Flow, suspect Income. If Net Income rises but OCF falls, investigate immediately. High quality earnings are cash-backed and recurring.\n</conclusion>"
    },
    {
        "Question": "A fund has high conviction in a short position but wants to reduce capital at risk. How would you structure a collar or other hedge?",
        "Reasoning": "<reason>\nShort the stock. Buy a Call (Cap upside risk). Sell a Put (Cap downside profit to fund the Call). This is a 'Short Collar'. It defines the max loss and max gain.\n</reason>\n<exception>\nBuying Calls is expensive (Skew). Selling Puts limits the 'home run' potential of the short. Alternatively, use 'Put Spreads' (Buy ATM Put / Sell OTM Put) instead of shorting the stock. Defined risk, leverage, no borrow cost.\n</exception>\n<tension>\nThis is a tension between 'Unlimited Risk' (Short stock) and 'Premium Cost' (Options).\n</tension>\n<categorization>\nThis is a 'Derivatives Structuring' problem.\n</categorization>\n<deconstruction>\nSynthetic Short: Buy Put, Sell Call. Similar to short stock but no borrow. If borrow is hard/expensive, synthetic is better.\n</deconstruction>\n<conclusion>\nUse a Put Spread. It creates a defined risk/reward payout. If you must short the stock, buy an OTM Call (protective call) to stop-loss the squeeze risk.\n</conclusion>"
    },
    {
        "Question": "Explain the concept of short-covering rallies. How would you exploit this phenomenon?",
        "Reasoning": "<reason>\nHeavily shorted stocks can spike when shorts rush to exit (buy back). This creates a feedback loop (Price Up -> Margin Call -> Buy -> Price Up). Known as a Squeeze.\n</reason>\n<exception>\nExploitation: 'Squeeze metric' (Short Interest / Float). If >20%, risk is high. Go Long heavily shorted stocks on positive momentum/news. Or, if you are short, cover into dips, don't press. Squeezes are mean-reverting eventually, but can be insolvent first.\n</exception>\n<tension>\nThis is a tension between 'Fundamental Bearishness' and 'Technical Bullishness'.\n</tension>\n<categorization>\nThis is a 'Trading Strategy' problem.\n</categorization>\n<deconstruction>\nSqueezes are 'Liquidity Crises' for shorts. Exploiting them is trading the flow, not the stock. GameStop is the archetype.\n</deconstruction>\n<conclusion>\nMonitor 'Days to Cover'. If high, be careful shorting. As a long trader, buy high-short-interest breakouts for explosive moves.\n</conclusion>"
    },
    {
        "Question": "How would you identify potential takeover targets? What due diligence would you conduct?",
        "Reasoning": "<reason>\nScreen for: Strategic value (Unique tech/customer base), Low valuation (EV/EBITDA), Clean balance sheet (easy LBO), Fragmented industry (Consolidation phase). Activist presence is a signal.\n</reason>\n<exception>\nDue Diligence: Poison pills? Dual-class shares (Founder control)? Antitrust hurdles? A cheap stock might be 'Un-acquirable' due to governance or toxic liabilities.\n</exception>\n<tension>\nThis is a tension between 'Intrinsic Value' and 'Strategic Control'.\n</tension>\n<categorization>\nThis is a 'Event Driven' problem.\n</categorization>\n<deconstruction>\nLook for 'Sum of the Parts' discount. If break-up value > market cap, a raider will notice. Also, check CEO age/tenure (looking for exit?).\n</deconstruction>\n<conclusion>\nFocus on strategic fit and governance. If a competitor *needs* this asset to survive, a bid is likely. If it's just cheap, it might stay cheap.\n</conclusion>"
    },
    {
        "Question": "A fund manager notices insider buying in a stock. Is this a bullish signal, and how would you validate the conviction?",
        "Reasoning": "<reason>\nBullish. 'Insiders sell for many reasons, but buy for only one: they think the price will go up'. It aligns management with shareholders.\n</reason>\n<exception>\nContext matters. Is it a token purchase (optical)? Is it part of a compensation plan? Buying by the CFO/CEO is more signal than a Director. Buying on the open market is better than option exercise. Size matters (relative to net worth).\n</exception>\n<tension>\nThis is a tension between 'Signal' and 'Noise/Window Dressing'.\n</tension>\n<categorization>\nThis is a 'Fundamental Analysis' problem.\n</categorization>\n<deconstruction>\nCluster buying (multiple insiders) is the strongest signal. Buying after a price drop (value) is stronger than buying at highs (momentum).\n</deconstruction>\n<conclusion>\nIt is a strong positive filter. Validate by checking the magnitude and the track record of that insider. If they are 'Smart Money', follow them.\n</conclusion>"
    },
    {
        "Question": "How would you manage concentration risk in a long/short portfolio with few positions?",
        "Reasoning": "<reason>\nHigh concentration (e.g., 10 positions) maximizes Alpha but maximizes Idiosyncratic Risk. Manage by: Position sizing limits (e.g., max 10% at cost), Stop-losses, and Correlation checks (don't hold 3 banks).\n</reason>\n<exception>\nWith few positions, you cannot diversify volatility. You must rely on 'Deep Due Diligence'. The risk management is *pre-trade* (knowing the asset), not *post-trade* (statistics). Hedging with Index Puts helps tail risk.\n</exception>\n<tension>\nThis is a tension between 'Conviction' and 'Survival'.\n</tension>\n<categorization>\nThis is a 'Portfolio Management' problem.\n</categorization>\n<deconstruction>\nConcentration requires 'Liquidity'. You must be able to exit. Concentrated illiquid positions are the widowmaker. \n</deconstruction>\n<conclusion>\nSet hard Drawdown limits per position. Hedge the Beta. Accept higher volatility as the price of higher returns, but eliminate ruin risk (leverage).\n</conclusion>"
    }
]

filename = 'tetralemma-reasoning-train.csv'
with open(filename, 'a', newline='', encoding='utf-8') as f:
    writer = csv.DictWriter(f, fieldnames=['Question', 'Reasoning'])
    for entry in new_entries:
        writer.writerow(entry)

print(f"Appended {len(new_entries)} entries (Batch 7) to {filename}")
